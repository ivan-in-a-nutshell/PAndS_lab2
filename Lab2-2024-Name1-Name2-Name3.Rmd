---
title: 'P&S-2022: Lab assignment 2'
author: "Name1, Name2, Name3"
output:
  html_document:
    df_print: paged
---

## General comments and instructions

-   Complete solution will give you **4 points** (working code with explanations + oral defense). Submission deadline **November 1, 2023, 22:00**\
-   The report must be prepared as an *R notebook*; you must submit to **cms** both the source *R notebook* **and** the generated html file\
-   At the beginning of the notebook, provide a work-breakdown structure estimating efforts of each team member\
-   For each task, include
    -   problem formulation and discussion (what is a reasonable answer to discuss);\
    -   the corresponding $\mathbf{R}$ code with comments (usually it is just a couple of lines long);\
    -   the statistics obtained (like sample mean or anything else you use to complete the task) as well as histograms etc to illustrate your findings;\
    -   justification of your solution (e.g. refer to the corresponding theorems from probability theory);\
    -   conclusions (e.g. how reliable your answer is, does it agree with common sense expectations etc)\
-   The **team id number** referred to in tasks is the **two-digit** ordinal number of your team on the list. Include the line **set.seed(team id number)** at the beginning of your code to make your calculations reproducible. Also observe that the answers **do** depend on this number!\
-   Take into account that not complying with these instructions may result in point deduction regardless of whether or not your implementation is correct.

Work breakdown:

-   Roman Leshchuk - task 1, task 4.1;

-   Ivan Zarytskyi - task 3, task 4.2;

-   Andrii Kryvyi - task 2, general conclusions.

------------------------------------------------------------------------

### Task 1

#### In this task, we discuss the $[7,4]$ Hamming code and investigate its reliability. That coding system can correct single errors in the transmission of $4$-bit messages and proceeds as follows:

-   given a message $\mathbf{m} = (a_1 a_2 a_3 a_4)$, we first encode it to a $7$-bit *codeword* $\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)$, where $G$ is a $4\times 7$ *generator* matrix\
-   the codeword $\mathbf{c}$ is transmitted, and $\mathbf{r}$ is the received message\
-   $\mathbf{r}$ is checked for errors by calculating the *syndrome vector* $\mathbf{z} := \mathbf{r} H$, for a $7 \times 3$ *parity-check* matrix $H$\
-   if a single error has occurred in $\mathbf{r}$, then the binary $\mathbf{z} = (z_1 z_2 z_3)$ identifies the wrong bit no. $z_1 + 2 z_2 + 4z_3$; thus $(0 0 0)$ shows there was no error (or more than one), while $(1 1 0 )$ means the third bit (or more than one) got corrupted\
-   if the error was identified, then we flip the corresponding bit in $\mathbf{r}$ to get the corrected $\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)$;\
-   the decoded message is then $\mathbf{m}^*:= (r_3r_5r_6r_7)$.

#### The **generator** matrix $G$ and the **parity-check** matrix $H$ are given by

$$  
    G := 
    \begin{pmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    \end{pmatrix},
 \qquad 
    H^\top := \begin{pmatrix}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1
    \end{pmatrix}
$$

#### Assume that each bit in the transmission $\mathbf{c} \mapsto \mathbf{r}$ gets corrupted independently of the others with probability $p = \mathtt{id}/100$, where $\mathtt{id}$ is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process $N$ times and find the estimate $\hat p$ of the probability $p^*$ of correct transmission of a single message $\mathbf{m}$. Comment why, for large $N$, $\hat p$ is expected to be close to $p^*$.\
2.  By estimating the standard deviation of the corresponding indicator of success by the standard error of your sample and using the CLT, predict the \emph{confidence} interval $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate $\hat p$ falls with probability at least $0.95$.\
3.  What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while transmitting a $4$-digit binary message. Do you think it is one of the known distributions?

#### You can (but do not have to) use the chunks we prepared for you

#### First, we set the **id** of the team and define the probability $p$ and the generator and parity-check matrices $G$ and $H$

```{r}
teamId <- 23

set.seed(teamId)
p <- teamId / 100

G <- matrix(c(
  1, 1, 1, 0, 0, 0, 0,
  1, 0, 0, 1, 1, 0, 0,
  0, 1, 0, 1, 0, 1, 0,
  1, 1, 0, 1, 0, 0, 1
), nrow = 4, byrow = TRUE)
H <- t(matrix(c(
  1, 0, 1, 0, 1, 0, 1,
  0, 1, 1, 0, 0, 1, 1,
  0, 0, 0, 1, 1, 1, 1
), nrow = 3, byrow = TRUE))
# cat("The matrix G is: \n")
# G
# cat("The matrix H is: \n")
# H
# cat("The product GH must be zero: \n")
# (G%*%H) %% 2
```

#### Next, generate the messages

```{r}
# generate N messages

message_generator <- function(N) {
  matrix(sample(c(0, 1), 4 * N, replace = TRUE), nrow = N)
}
N <- 1000000
messages <- message_generator(N)
codewords <- (messages %*% G) %% 2
```

#### Generate random errors; do not forget that they occur with probability $p$! Next, generate the received messages

```{r}
errors <- matrix(rbinom(N * 7, size = 1, prob = p), nrow = N, ncol = 7)
received <- (codewords + errors) %% 2
```

#### The next steps include detecting the errors in the received messages, correcting them, and then decoding the obtained messages. After this, you can continue with calculating all the quantities of interest

```{r}
syndromes <- (received %*% H) %% 2

syndrome_vals <- syndromes[,1] + 2*syndromes[,2] + 4*syndromes[,3]

corrected <- received
for (i in 1:N) {
  pos <- syndrome_vals[i]
  if (pos != 0) {
    corrected[i, pos] <- (corrected[i, pos] + 1) %% 2
  }
}

msg_idx <- c(3, 5, 6, 7)
decoded_messages <- corrected[, msg_idx, drop = FALSE]

decoded_ok <- apply(decoded_messages == messages, 1, all)

num_errors <- rowSums(errors)

cat("Total simulated messages:", N, "\n")
print(table(num_errors))
cat("\nTable: num_errors vs decoded_ok (counts)\n")
print(table(num_errors, decoded_ok))

ok_le1 <- if (any(num_errors <= 1)) all(decoded_ok[num_errors <= 1]) else TRUE
ok_gt1 <- if (any(num_errors > 1)) all(!decoded_ok[num_errors > 1]) else TRUE

cat("\nAll decoded correctly when num_errors <= 1 ?", ok_le1, "\n")
cat("All decoded incorrectly when num_errors > 1 ?", ok_gt1, "\n")

viol_le1 <- which((num_errors <= 1) & (!decoded_ok))
viol_gt1 <- which((num_errors > 1) & (decoded_ok))

cat("\nViolations: cases with <= 1 error but decoded incorrectly:", length(viol_le1), "\n")
if (length(viol_le1) > 0) print(head(viol_le1))

cat("Violations: cases with > 1 error but decoded correctly:", length(viol_gt1), "\n")
if (length(viol_gt1) > 0) print(head(viol_gt1))
```

Lets calculate theoretical probability $p^*$ of correct transmission (correct transmission means that transmitted message after correction is same as original). It is true if there are exactly 0 or 1 error in message during transmission of message (7 bits). Let r.v. $X$ denote number or errors in message, then:

$X \sim B(7, p)$

$P(X <= 1) = P(X = 0) + P(X = 1) = {7 \choose 0} \cdot p^{0} \cdot (1 - p)^{7} + {7 \choose 1} \cdot p^{1} \cdot (1 - p)^{6} = (1 - p)^{7} + 7 \cdot p \cdot (1 - p)^{6}$

Lets compare it with our estimate, which is just mean of realizations of indicator r.v. (which equals 1 if transmission is correct, 0 otherwise):

```{r}
theoretical_p <- (1 - p) ^ 7 + 7 * p * (1 - p) ^ 6
cat("Theoretical probability of correct transmission:", theoretical_p, "\n")

estimate_p <- mean(decoded_ok)
cat("Estimate of this probability:", estimate_p, "\n")
```

According to the law of large numbers, estimated probability $\hat{p}$ converges to theoretical probability $p^*$ when N is large enough.

Now lets calculate standard error (variance of Bernoulli r.v. with parameter $p$ is $p(1-p)$, standard error is square root of variance divided by square root of N):

```{r}
estimate_se <- sqrt(estimate_p * (1 - estimate_p) / N)
cat(sprintf("Estimate standard error = %.6f\n", estimate_se))
```

Now, according to CLT, distribution of N i.i.d. Bernoulli r.v. (with known mean and variance, denote variable with this joint distribution $Y$ of distributions $X$) converges to $N(\mu, \sigma^2)$. And we can compute confidence interval bounds from formula ($Y$ must be normalized firstly): $P(z_v \le \frac{XN - \mu N}{\sigma \sqrt N}  \le z_v) = 0.95$, or $(\mu - z_v \cdot SE \le Y \le \mu + z_v \cdot SE) = 0.95$. We are discarding half of $0.05$ from both tails of distribution, so $z_v$ is value of z-function with argument $1 - \frac{0.05}{2}$:

```{r}
ci_probability = 0.95
discard_from_both_tails <- (1 - ci_probability) / 2
z_v <- qnorm(1 - discard_from_both_tails)
epsilon = z_v * estimate_se
ci_lower <- estimate_p - epsilon
ci_upper <- estimate_p + epsilon
ci_lower <- max(0, ci_lower)
ci_upper <- min(1, ci_upper)

cat(sprintf("z_v: %.6f\n", z_v))
cat(sprintf("95%% confidence interval epsilon: %.6f\n", epsilon))
cat(sprintf("95%% confidence interval: (%.6f, %.6f)\n", ci_lower, ci_upper))
```

Now lets calculate what choice of $N$ guarantees that $\varepsilon \le K; K = 0.03$. If $\varepsilon \le K$, then:

$SE * z_v \le K$

Or:

$\sqrt{\frac{\hat{p}(1 - \hat{p})}{N}} * z_v \le K$

$\frac{\hat{p}(1 - \hat{p})}{N} \le (\frac{K}{z_v})^2$

${N} \ge \frac{\hat{p}(1 - \hat{p})z_v^2}{K^2}$

```{r}
K = 0.03
min_n = ceiling(estimate_p * (1 - estimate_p) * z_v ^ 2 / K ^ 2)
cat(sprintf("Minimum N if epsilon <= %.2f: %.0f\n", K, min_n))
```

Now lets draw histogram of errors per transmitted message (we found it earlier, but for transformed 7-bit messages, not for only for 4 data bits, so now lets find for data bits):

```{r}
errors_per_decoded <- rowSums(messages != decoded_messages)
decoded_per_errors <- table(errors_per_decoded)

barplot(decoded_per_errors,
        main="Histogram of number of errors in decoded 4-digit messages", xlab="Number of errors", ylab="Number of messages", col="skyblue")
```

If our messages would be without corrections, then our distribution would be just $B(4, p)$ by definition - we make error in each bit independently with probability $p$, and parity bits does not matter for us. But after correction, if there was only one error or no errors in all 7 bits, it becomes zero (corrected). But if there was for example two or more errors, we cannot guarantee that it will correct any stable number of incorrect bits. It doesn't correspond to any well-known distribution. There is even a probability that with more than two errors it will not be able to identify that there is any error: error can change both data and parity bits.

#### Results

In this task, we firstly simulated encoding and decoding messages to 7,4-hamming code and correcting single errors in messages, calculated estimated and theoretical probability of event that decoded and corrected message would match original, also calculated standard error of indicator r.v. that represents this event. Then we analyzed bounds in which will be estimated probability of decoding message correctly for some set of N messages (estimated probability will be in this bounds with certain probability that we took into consideration while calculations), used CLT, estimated mean and standard deviation, standard error for this. Also we showed that distribution of number of decoded messages per number of errors in message does not represent any well-known distribution.

### Task 2.

In this task, we discuss a real-life process that is well modelled by a Poisson distribution. As you remember, a Poisson random variable describes occurrences of rare events, i.e., counts the number of successes in a large number of independent random experiments. One of the typical examples is the **radioactive decay** process.

Consider a sample of radioactive element of mass $m$, which has a big *half-life period* $T$; it is vitally important to know the probability that during a one second period, the number of nuclei decays will not exceed some critical level $k$. This probability can easily be estimated using the fact that, given the *activity* ${\lambda}$ of the element (i.e., the probability that exactly one nucleus decays in one second) and the number $N$ of atoms in the sample, the random number of decays within a second is well modelled by Poisson distribution with parameter $\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro constant, and $M$ is the molar (atomic) mass of the element. The activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is measured in seconds.

Assume that a medical laboratory receives $n$ samples of radioactive element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life period $T = 30.1$ years and mass $m = \mathtt{team\, id \,number} \times 10^{-6}$ g each. Denote by $X_1,X_2,\dots,X_n$ the **i.i.d. r.v.**'s counting the number of decays in sample $i$ in one second.

1.  Specify the parameter of the Poisson distribution of $X_i$ (you'll need the atomic mass of *Cesium-137*)\
2.  Show that the distribution of the sample means of $X_1,\dots,X_n$ gets very close to a normal one as $n$ becomes large and identify that normal distribution. To this end,
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical cumulative distribution function $\hat F_{\mathbf{s}}$ of $\mathbf{s}$;
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} $F$ of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $\hat F_{\mathbf{s}}$ and plot both **c.d.f.**'s on one graph to visualize their proximity (use the proper scales!);
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.\
3.  Calculate the largest possible value of $n$, for which the total number of decays in one second is less than $8 \times 10^8$ with probability at least $0.95$. To this end,
    -   obtain the theoretical bound on $n$ using Markov inequality, Chernoff bound and Central Limit Theorem, and compare the results;\
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sum $s=x_1 + \cdots +x_n$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    -   calculate the number of elements of the sample which are less than critical value ($8 \times 10^8$) and calculate the empirical probability; comment whether it is close to the desired level $0.95$

The derivation of the Poisson distribution parameter is as follows: $$
\mu = N\cdot\lambda = \frac{m}{M}\cdot N_A \cdot \frac{\log 2}{T} = \frac{m \cdot N_A \cdot \log 2}{M \cdot T}
$$ Atomic mass of `Cesium-137` is $M=136.907089$.

```{r}
teamId <- 23
set.seed(teamId)
T <- 30.1 * 60 * 60 * 24 * 365 # Half-life period
m <- teamId * 10^-6 # Mass of the sample
M <- 136.907089 # For Cesium-137
lambda <- log(2) / T # Activity
N <- m / M * 6 * 10^23 #  Number of atoms
mu <- N * lambda
print(sprintf("Poisson(%.3e)", mu))
K <- 1e3
n <- 5
sample_means <- colMeans(matrix(rpois(n * K, lambda = mu), nrow = n))
```

#### Next, calculate the parameters of the standard normal approximation

$$
\overline{X} = \frac{1}{n}\sum_{i=1}^nX_i
$$

$$
E[\overline{X}] = \sum_{i=1}^nE[\frac{X_i}{n}] = n \cdot \frac{E[X_1]}{n} = N\cdot\lambda \\
Var(\overline{X}) = \sum_{i=1}^nVar(\frac{X_i}{n} = n \cdot \frac{Var(X_1)}{n^2} = \frac{N \cdot \lambda}{n}
$$ We can approximate the $\overline{X}$ using CLT:

$$
\overline{X} \approx Y,\quad Y \sim \mathcal{N}(E[\overline{X}], Var(\overline{X})) \\
Y \sim \mathcal{N}(N \cdot \lambda, \frac{N\cdot \lambda}{n})
$$

```{r}
mu <- N * lambda
sigma <- sqrt(N * lambda / n)
```

#### We can now plot ecdf and cdf

```{r}
xlims <- c(mu - 3 * sigma, mu + 3 * sigma)
Fs <- ecdf(sample_means)
plot(Fs,
  xlim = xlims,
  ylim = c(0, 1),
  col = "blue",
  lwd = 2,
  main = "Comparison of ecdf and cdf"
)
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
```

The difference is as follows:

```{r}
x <- sort(unique(sample_means))
print(sprintf("max|Fs(x) - Fy(x)|=%f", max(abs(Fs(x) - pnorm(x, mean = mu, sd = sigma)))))
```

##### Repeating for distinct values of n

```{r}
mu <- N * lambda
print(sprintf("Poisson(%.3e)", mu))
for (n in c(5, 10, 50)) {
  sigma <- sqrt(N * lambda / n)
  sample_means <- colMeans(matrix(rpois(n * K, lambda = mu), nrow = n))
  xlims <- c(mu - 3 * sigma, mu + 3 * sigma)
  Fs <- ecdf(sample_means)
  plot(Fs,
    xlim = xlims,
    ylim = c(0, 1),
    col = "blue",
    lwd = 2,
    main = sprintf("Comparison of ecdf and cdf for n=%d", n)
  )
  curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
  x <- sort(unique(sample_means))
  print(sprintf("n=%d: max|Fs(x) - Fy(x)|=%f; mean=%f; sd=%f", n, max(abs(Fs(x) - pnorm(x, mean = mu, sd = sigma))), mean(sample_means), sd(sample_means)))
}
```

We can see that difference between normal approximation and empirical value doesn't change except small fluctuations, mean also sits around the mean of our original distribution, however standard deviation is getting smaller with n getting larger. This directly follows from the Law of large numbers.

#### Calculating largest n

Let $S_n = \sum_{i=1}^n X_i$. We can evaluate expected value and variance:

$$
E[S_n] = \sum_{i=1}^nE[X_i] = nN\lambda \\
Var(S_n) = \sum_{i=1}^nVar(X_i) = n N\lambda \\
\theta = 8\cdot10^8
$$

```{r}
theta <- 8 * 10^8
mu <- N * lambda
n <- 1
K <- 10^6
while (TRUE) {
  sample_sums <- colSums(matrix(rpois(n * K, lambda = mu), nrow = n))
  prob <- sum(sample_sums < theta) / length(sample_sums)
  if (prob <= .95) {
    break
  }
  n <- n + 1
}
print(sprintf("n≤%d", n))
```

#### Theoretical bounds

##### Markov's inequality

$$
P(S_n \geq \theta) \leq \frac{E[S_n]}{\theta} \\
-P(S_n\geq \theta) \geq -\frac{n N \lambda}{\theta} \\
P(S_n < \theta) \geq 1 - \frac{n N \lambda}{\theta} > 0.95 \\
1 - \frac{n N \lambda}{\theta} > 0.95 \\
\frac{n N  \lambda}{\theta} < 0.05 \\
\frac{n N\lambda}{\theta} < 0.05 \\
n < \frac{0.05\cdot\theta}{N\lambda} \\
n \leq \lceil \frac{0.05\cdot\theta}{N\lambda} - 1 \rceil
$$

```{r}
print(sprintf("n≤%d", ceiling((.05 * theta) / (N * lambda) - 1)))
```

##### Chernoff bound

$$
P(S_n \geq \theta) \leq \min_{s\in\mathbb{R}} e^{-s \theta}M(s) \\
M(s) = \prod_{i=1}^nM_{X_i}(s) = \prod_{i=1}^n e^{N\lambda(e^s-1)} = e^{nN\lambda(e^s-1)} \\
P(S_n \geq \theta) \leq \min_{s\in\mathbb{R}} e^{-s \theta}e^{nN\lambda(e^s-1)} \\
-P(S_n \geq \theta) \geq -\min_{s\in\mathbb{R}} e^{-s \theta}e^{nN\lambda(e^s-1)} \\
P(S_n < \theta) \geq 1-\min_{s\in\mathbb{R}} e^{-s \theta}e^{nN\lambda(e^s-1)} \\
P(S_n < \theta) \geq 1-\min_{s\in\mathbb{R}} \exp(-s \theta+nN\lambda(e^s-1)) \\
P(S_n < \theta) \geq 1-\min_{s\in\mathbb{R}} \exp(-\theta s+nN\lambda e^s-nN\lambda) \\
$$

To find $\min_{s\in\mathbb{R}}\big[f(s) = \exp(-\theta s+nN\lambda e^s-nN\lambda)\big]$ we apply calculus:

$$
f'(s^*) = (-\theta + nN\lambda e^{s^*})\exp(-\theta s^*+nN\lambda e^{s^*}-nN\lambda) = 0 \\
-\theta + nN\lambda e^{s^*} = 0 \\
nN\lambda e^{s^*} = \theta \\
e^{s^*} = \frac{\theta}{nN\lambda} \\
s^* = \log\frac{\theta}{nN\lambda} \\
$$ But since $s > 0$ we need to constrain $s^* > 0$:

$$
\log\frac{\theta}{nN\lambda} > 0 \\
\frac{\theta}{nN\lambda} > 1 \\
nN\lambda < \theta \\
n < \frac{\theta}{N\lambda} \\
$$

It means that we have to work in the range $s\in\Big(0, \frac{\theta}{N\lambda}\Big)$

From $s^* = \arg \min f(s)$, we get that:

$$
\min_{s\in\mathbb{R}}f(s) = f(s^*) = f(\log\frac{\theta}{nN\lambda}) = \exp(-\theta \log\frac{\theta}{nN\lambda}+nN\lambda \frac{\theta}{nN\lambda}-nN\lambda) =\\
= \exp(-\theta \log\frac{\theta}{nN\lambda}+\theta-nN\lambda)
$$

So Chernoff bound gets us following:

$$
P(S_n < \theta) \geq 1-\exp(-\theta \log\frac{\theta}{nN\lambda}+\theta-nN\lambda) > 0.95 \\
1-\exp(-\theta \log\frac{\theta}{nN\lambda}+\theta-nN\lambda) > 0.95 \\
\exp(-\theta \log\frac{\theta}{nN\lambda}+\theta-nN\lambda) < 0.05 \\
-\theta \log\frac{\theta}{nN\lambda}+\theta-nN\lambda < \log0.05 \\
-\theta (\log \theta - \log{nN\lambda})+\theta-nN\lambda < \log0.05 \\
-\theta\log \theta + \theta\log{nN\lambda}+\theta-nN\lambda < \log0.05 \\
\theta\log{nN\lambda}-nN\lambda < \log0.05+\theta\log \theta-\theta
$$

This inequality cannot be solved algebraically, so we use binary search algorithm to find largest $n$ that satisfies the inequality.

Inequality definition:

```{r}
rhs <- log(0.05) + theta * log(theta) - theta
lhs <- function(n) {
  t <- n * N * lambda
  return(theta * log(t) - t)
}
```

Search algorithm:

```{r}
left <- 1
right <- theta / (N * lambda)
while (left <= right) {
  mid <- ceiling((left + right) / 2)
  lhs_cur <- lhs(mid)
  if (lhs_cur < rhs) {
    n <- mid
    left <- mid + 1
  } else {
    right <- mid - 1
  }
}

print(sprintf("n≤%d", n))
```

##### Normal approximation

Using CLT we can state that (recall $E[S_n]$ and $Var(S_n)$):

$$
S_n \approx \sqrt{nN\lambda}Z + nN\lambda \\
Z \sim \mathcal{N}(0, 1)
$$

Using this approximation we can try to approximate the probability:

$$
P(S_n < \theta) \approx P( \sqrt{nN\lambda}Z + nN\lambda < \theta) \\
P(S_n < \theta) \approx P( Z < \frac{\theta-nN\lambda}{\sqrt{nN\lambda}}) \\
P(S_n < \theta) \approx \Phi(\frac{\theta-nN\lambda}{\sqrt{nN\lambda}})\\
P(S_n < \theta) > 0.95 \\
\Phi(\frac{\theta-nN\lambda}{\sqrt{nN\lambda}}) > 0.95\\
\frac{\theta-nN\lambda}{\sqrt{nN\lambda}} > \Phi^{-1}(0.95) \\
\frac{\theta-nN\lambda}{\sqrt{nN\lambda}} > z_{0.95} \\
\theta-nN\lambda > z_{0.95}\sqrt{nN\lambda} \\
u := \sqrt{nN\lambda} \\
\theta - u^2 > z_{0.95}u \\
u^2 + z_{0.95}u - \theta < 0 \\
u \in \Big(\frac{-z_{0.95} - \sqrt{(z_{0.95})^2 + 4\theta}}{2}, \frac{-z_{0.95} + \sqrt{(z_{0.95})^2 + 4\theta}}{2}\Big) \\
\sqrt{nN\lambda} \in \Big(\frac{-z_{0.95} - \sqrt{(z_{0.95})^2 + 4\theta}}{2}, \frac{-z_{0.95} + \sqrt{(z_{0.95})^2 + 4\theta}}{2}\Big) \\
nN\lambda \in \Big[0, (\frac{-z_{0.95} + \sqrt{(z_{0.95})^2 + 4\theta}}{2})^2\Big) \\
n \in \Big[0, (\frac{(-z_{0.95} + \sqrt{(z_{0.95})^2 + 4\theta})^2}{4N\lambda}) \Big) \\
n \leq \lceil (\frac{(-z_{0.95} + \sqrt{(z_{0.95})^2 + 4\theta})^2}{4N\lambda})  - 1 \rceil
$$

```{r}
z95 <- qnorm(.95)
n <- ceiling((-z95 + sqrt(z95^2 + 4 * theta))^2 / (4 * N * lambda) - 1)
print(sprintf("n≤%d", n))
```

##### Results

We can see that the Markov's inequality is the most conservative one geaving the result of $n\leq0$ (basically $n=0$). While the Chernoff bound is the most precise and coincides with the empirical estimate $n \leq 11$. The normal approximation through CLT works here well giving the result of $n \leq 10$ which is very close to the real bound, yet is easier to solve than the Chernoff bound, which required numerical solution for the inequality.

------------------------------------------------------------------------

### Task 3.

#### In this task, we use the Central Limit Theorem approximation for continuous random variables.

#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by $X_k$ the random time between the $(k-1)^{\mathrm{st}}$ and $k^{\mathrm{th}}$ click of the counter.

1.  Show that the distribution of the sample means of $X_1, X_2,\dots,X_n$ gets very close to a normal one (which one?) as $n$ becomes large. To this end,
    -   simulate the realizations $x_1,x_2,\dots,x_n$ of the \textbf{r.v.} $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the \emph{empirical cumulative distribution} function $F_{\mathbf{s}}$ of $\mathbf{s}$;\
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $F_{\mathbf{s}}$ of and plot both \textbf{c.d.f.}'s on one graph to visualize their proximity;\
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;\
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.
2.  The place can be considered safe when the number of clicks in one minute does not exceed $100$. It is known that the parameter $\nu$ of the resulting exponential distribution is proportional to the number $N$ of the radioactive samples, i.e., $\nu = \nu_1*N$, where $\nu_1$ is the parameter for one sample. Determine the maximal number of radioactive samples that can be stored in that place so that, with probability $0.95$, the place is identified as safe. To do this,
    -   express the event of interest in terms of the \textbf{r.v.} $S:= X_1 + \cdots + X_{100}$;\

    -   obtain the theoretical bounds on $N$ using the Markov inequality, Chernoff bound and Central Limit Theorem and compare the results;\

    -   with the predicted $N$ and thus $\nu$, simulate the realization $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum $S = X_1 + \cdots + X_{100}$;\

    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the $100^{\mathrm{th}}$ click;\

    -   estimate the probability that the location is identified as safe and compare to the desired level $0.95$

        #### Part 1

For $X_i​∼Exp(\nu_1​)$:

-   Mean: $E[X_i​]=\frac{1}{\nu_1}$

-   Variance: $Var(X_i​)=\frac{1}{\nu_1^2}$

Then by CLT $\bar{X_n}$ approaches $$ N(\mu, \sigma_n^2) $$:

where

$$
\mu = E(\bar{X}_n) = E(X_i) = \frac{1}{\nu_1}
$$ $$
\sigma_n^2 = Var(\bar{X}_n) = \frac{Var(X_i)}{n} = \frac{1}{n\nu_1^2}
$$

Therefore for $$ \nu_1 = 33 $$

$$
N(\frac{1}{33}, \frac{1}{n33^2})
$$

#### First, generate samples an sample means:

```{r}
set.seed(23)
```

```{r}
nu1 <- 23 + 10
K <- 1e3
n_5 <- 5
sample_means <- colMeans(matrix(rexp(n_5*K, rate = nu1), nrow=n_5))

mu <- 1 / nu1
sigma <- 1 / (nu1 * sqrt(n_5))

xlims <- c(mu-4*sigma,mu+4*sigma)
Fs <- ecdf(sample_means)

plot(Fs, 
     xlim = xlims, 
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)

x <- seq(xlims[1], xlims[2], length.out = 1000)
max(abs(Fs(x)-pnorm(x, mean = mu, sd = sigma)))
```

```{r}
n_10 <- 10
sample_means <- colMeans(matrix(rexp(n_10*K, rate = nu1), nrow=n_10))

mu <- 1 / nu1
sigma <- 1 / (nu1 * sqrt(n_10))

xlims <- c(mu-4*sigma,mu+4*sigma)
Fs <- ecdf(sample_means)

plot(Fs, 
     xlim = xlims, 
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)

x <- seq(xlims[1], xlims[2], length.out = 1000)
max(abs(Fs(x)-pnorm(x, mean = mu, sd = sigma)))
```

```{r}
n_50 <- 50
sample_means <- colMeans(matrix(rexp(n_50*K, rate = nu1), nrow=n_50))

mu <- 1 / nu1
sigma <- 1 / (nu1 * sqrt(n_50))

xlims <- c(mu-4*sigma,mu+4*sigma)
Fs <- ecdf(sample_means)

plot(Fs, 
     xlim = xlims, 
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)

x <- seq(xlims[1], xlims[2], length.out = 1000)
max(abs(Fs(x)-pnorm(x, mean = mu, sd = sigma)))
```

As n increases, the maximal difference between the empirical CDF (blue line) and the theoretical normal CDF (red line) decreases.

This empirically demonstrates the CLT: as the sample size n increases, the distribution of the sample means converges to the predicted normal distribution.

#### Part 2

Let $$C(t) - \#\text{clicks in t minutes}$$ $$
C(t) \sim P(\lambda t);\nu = \nu1N=33N;\lambda=\nu
$$ $$
P(C(1) \le 100) \ge 0.95
$$ $$
\text{Let } S_{100} = X_1 + X_2 + ...+X_{100}(\text{Where } X_i \sim E(\nu))
$$ $$
C(1) < 100 \Rightarrow S_{100} > 1 \\ C(1) \le 100 \Rightarrow S_{101} > 1 \approx S_{100} > 1
$$ $$
P(S_{100} > 1) \ge 0.95 \Rightarrow P(S_{100} \le 1) \le 0.05
$$ $$
S_{100} \sim \Gamma(100, \nu) \\ \mu_S = E(S_{100}) = \frac{100}{\nu} \\ \sigma_{S}^2=\frac{100}{\nu^2}
$$

$$
\text{Markov: For better calculations lets introduce Y: } \\
Y = \#\text{clicks in 1 minute} \Rightarrow \text{We want: } P(Y \le 100) \ge 0.95 \\
\text{To use Markov's equations let's use the complement} \\
1 - P(Y > 100) \ge 0.95 \Rightarrow P(Y > 100) \le 0.05 \Rightarrow P(Y \ge 101) \le 0.05 \\ 
P(Y \ge 101) \le \frac{N\nu1}{101} \Rightarrow \frac{N\nu1}{101} \le 0.05 \\
N \le \frac{0.05 * 33}{101} \Rightarrow N \le 0.01 \\
\text{Markov says: } N_{max}=0 \\
$$

$$
\text{CLT:} \\
S_{100} \sim N(\mu_S, \sigma_S^2) \text{ by CLT} \\
S_{100} \sim N(\frac{100}{\nu},\frac{100}{\nu^2}) \\
P(S_{100} \le 1) \le 0.05 \\
P(Z \le \frac{1-\nu_S}{\sigma_S}) \le 0.05 \text{ (}P(Z \le -1.645) \approx 0.05 \text{ from table}\text{)} \\
\frac{1-\nu_S}{\sigma_S} \le -1.645 \\
\frac{1-\frac{100}{\nu}}{\frac{10}{\nu}} \le -1.645 \\
... \\
\nu \le 83.55 \\
33N \le 83.55 \\
N \le 2.53 \Rightarrow N_{max}=2
$$

$$
\text{Chernoff:} \\
P(S_{100} \le 1) \le e^{-t}M_{S_{100}}(t) \\
M_{S_{100}}(t) = E(e^{tX_i}) = \frac{\nu}{\nu - t} (t < \nu) \\
M_{S_{100}}(t) = (M_{X_i}(t))^{100} = \left( \frac{\nu}{\nu - t} \right)^{100} \\
P(S_{100} \le 1) \le e^{-t} \left( \frac{\nu}{\nu - t} \right)^{100} (t<0) \\
\text{Let: } f(t) = -t + 100(\ln\nu - \ln(\nu - t)) \\
f'(t) = -1 + \frac{100}{\nu - t} = 0 \Rightarrow \nu < 100 \\
33N < 100 \Rightarrow N \le 3 \\
P(S_{100} \le 1) \le e^{100 - \nu} \left( \frac{\nu}{100} \right)^{100} \\
\text{For N = 2: } \\
P(S_{100} \le 1) \le e^{34} 0.66^{100} \\
\ln(P(S_{100} \le 1)) \approx 34 + 100\ln0.66 \\
\ln(P(S_{100} \le 1)) \approx -7.55 \\
P(S_{100} \le 1) \le e^{-7.55} \\
$$

$$
P(S_{100} \le 1) \le 0.0005 \text{ Therefore N=2 is good} \\
\text{N=3: } \\
P(S_{100} \le 1) \le e^1 0.99^{100} \\
P(S_{100} \le 1) \approx e^1 e^{-1} \Rightarrow P(S_{100} \le 1) \approx 1 \ge 0.05 \text{ So n=3 is bad} \\
\Rightarrow N_{max} = 2
$$

```{r}
set.seed(99)
N <- 2
nu <- nu1 * N
n_clicks <- 100

sample_sums <- colSums(matrix(rexp(n_clicks * K, nu), nrow = n_clicks))

prob_safe <- mean(sample_sums > 1)

print(paste("Predicted N_max (from CLT and Chernoff):", N))
print(paste("Resulting parameter nu:", nu))
print(paste("Simulated probability of being safe (P(S_100 > 1)):", prob_safe))
print(paste("Desired probability level:", 0.95))
```

Both CLT and Chernoff gave N=2, which in our case gave probability 1. We can check that by setting N to 3 the probability plummets to \~0.5

### Task 4.

**This task consists of two parts:**

1.  **In this part, we discuss independence of random variables and its moments: expectation and variance.**

    1.  Suppose we have a random variable $X$. Explain why $\mathbb{E}(\frac{1}{X}) \neq \frac{1}{\mathbb{E(X)}}$;

    Because in general, for nonlinear function $f$: $E(f(x)) \neq f(E(X))$, and function f(X) = 1/X is not linear.

    2.  Let $X \sim \mathscr{N}(\mu,\sigma^2)$ with $\mu = teamidnumber$ and $\sigma^{2} = 2\times teamidnumber+7$. Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ and $y_1,y_2,\dots,y_{100}$ of $Y := \frac{1}{X}$ to calculate the values of $\frac{1}{\overline{\textbf{X}}}$ and $\overline{\textbf{Y}}$. Comment on the received results;

    ```{r}
    teamId <- 23

    set.seed(23)

    mu <- teamId
    sigma <- sqrt(2 * teamId + 7)
    N <- 100

    X <- rnorm(N, mean = mu, sd = sigma)
    Y <- 1 / X

    cat("1/E(X) = ", 1 / mean(X), "\n")
    cat("E(Y) = E(1/X) = ", mean(Y), "\n")
    ```

    These values are not equal because $\mathbb{E}(\frac{1}{X}) \neq \frac{1}{\mathbb{E(X)}}$.

    3.  Let $X$ and $Y$ be exponentially distributed r.v.'s with parameter $\lambda = 2$. Set $Z := \log{X} + 5$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Y$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Z$. Explain the results. Comment on the difference of relations between the pairs of random variables. Which pair of r.v.'s is dependent and which one is similar?

    ```{r}
    lambda <- 2
    N <- 100

    X <- rexp(N, rate = lambda)
    Y <- rexp(N, rate = lambda)
    Z <- log(X) + 5

    par(mfrow = c(1,2))
    plot(X, Y, main="Scatterplot X vs Y")
    qqplot(X, Y, main="QQ-plot X vs Y"); abline(0, 1, col="red")

    par(mfrow = c(1, 2))
    plot(X, Z, main="Scatterplot X vs Z")
    qqplot(X, Z, main="QQ-plot X vs Z"); abline(0, 1, col="red")
    ```

    Scatterplot of X and Y is pretty random, that means that there is no dependency between X and Y. QQ-plot of X and Y is aligned near graph y = x, it means that distributions X and Y are similar (because quantiles are similar).

    On the other hand, on scatterplot of X and Z we can see shape of function (precisely z = log(x) + 5), so distributions are dependent on each other. Same for QQ-plot, we can see shape of the same function z = log(x) + 5, which is far from z = x, it shows that distributions of X and Z are different.

    ```         
    ------------------------------------------------------------------------
    ```

2.  You toss a fair coin three times and a random variable $X$ records how many times the coin shows Heads. You convince your friend that they should play a game with the following payoff: every round (equivalent to three coin tosses) will cost £$1$. They will receive £$0.5$ for every coin showing Heads. What is the expected value and the variance of the random variable $Y := 0.5X-1$?

    To answer this,

    1.  Explain what type of random variable is X:

        -   Normally distributed

        -   Binomially distributed

        -   Poisson distributed

        -   Uniformly distributed

    2.  What are the expected value and variance of X? Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ to calculate the values of sample mean $\overline{\mathbf{X}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(x_i - \overline{x})^{2}}}{n-1}$. Comment on the results;

```{r}
set.seed(23)
n_sims <- 100
n_trials <- 3
p_success <- 0.5

x_samples <- rbinom(n_sims, n_trials, p_success)
x_mean <- mean(x_samples)
x_var <- var(x_samples)

print(paste("Theoretical E[X]:", 1.5))
print(paste("Sample Mean of X:", x_mean))
print("---")
print(paste("Theoretical Var(X):", 0.75))
print(paste("Sample Variance of X:", x_var))
```

3.  What are the expected value and variance of Y? Simulate realizations $y_1,y_2,\dots,y_{100}$ of $Y$ to calculate the values of sample mean $\overline{\mathbf{Y}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(y_i - \overline{y})^{2}}}{n-1}$. Comment on the results;

```{r}
n_sims <- 100
n_trials <- 3
p_success <- 0.5

y_samples <- 0.5 * x_samples - 1
y_mean <- mean(y_samples)
y_var <- var(y_samples)

print(paste("Theoretical E[X]:", -0.25))
print(paste("Sample Mean of X:", y_mean))
print("---")
print(paste("Theoretical Var(X):", 0.1825))
print(paste("Sample Variance of X:", y_var))
```

All simulated values are very close to the theoretically calculated ones.

The negative expected value E[Y]=−0.25 means that this game is unfavorable for our friend. On average, he will lose 25 cents each round.

------------------------------------------------------------------------

### General summary and conclusions

We have worked on some simulations of random variables and connected empirical results with theoretical. We have applied the CLT, used Markov's inequality and Chernoff bounds. The hardest part was that some mathematical equations were not solvable algebraically, which required numerical approach. In general, we conclude that simulations align with the results from theoretical prospective.
